# üìä Documentaci√≥n del Sistema de Scraping - Marketplace Admin

## üìã √çndice
1. [Visi√≥n General](#visi√≥n-general)
2. [Arquitectura](#arquitectura)
3. [Componentes](#componentes)
4. [API Integration](#api-integration)
5. [Gu√≠a de Uso](#gu√≠a-de-uso)
6. [Desarrollo](#desarrollo)
7. [Configuraci√≥n](#configuraci√≥n)
8. [Troubleshooting](#troubleshooting)

## üéØ Visi√≥n General

El Sistema de Scraping es un m√≥dulo integrado en el Marketplace Admin que permite monitorear y gestionar la recolecci√≥n autom√°tica de productos del mercado argentino. Este sistema se conecta con el servicio de scraping Python existente para proporcionar una interfaz visual completa.

### Caracter√≠sticas Principales

- **Dashboard de M√©tricas**: Vista en tiempo real del estado del sistema
- **Monitor de Jobs**: Seguimiento de trabajos de scraping activos
- **Gesti√≥n de Fuentes**: Control de 30+ sitios web argentinos
- **Programaci√≥n Autom√°tica**: Configuraci√≥n de horarios con cron
- **Historial Completo**: Registro de todas las ejecuciones

### Flujo de Datos

```
Marketplace Admin (Frontend) 
    ‚Üì
Scraper Service (Python - Puerto 8086)
    ‚Üì
MongoDB (Almacenamiento de productos)
```

## üèóÔ∏è Arquitectura

### Estructura del M√≥dulo

```
services/saas-mt-marketplace-admin/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/scraper/                    # P√°ginas del m√≥dulo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                    # Dashboard principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sources/page.tsx            # Gesti√≥n de fuentes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schedule/page.tsx           # Programaci√≥n
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ history/page.tsx            # Historial
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ components/scraper/             # Componentes React
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ScraperDashboard.tsx       # Dashboard con m√©tricas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ JobMonitor.tsx              # Monitor de jobs activos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SourceManager.tsx           # Gesti√≥n de fuentes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ScheduleConfig.tsx          # Configuraci√≥n cron
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ScrapingHistory.tsx         # Tabla de historial
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ hooks/scraper/                  # Custom hooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useScraperDashboard.ts      # Estado del dashboard
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useScraperTargets.ts        # Gesti√≥n de fuentes
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ lib/api/scraper/                # Cliente API
‚îÇ       ‚îî‚îÄ‚îÄ scraper-api.ts              # Comunicaci√≥n con backend
```

### Integraci√≥n con el Sistema

El m√≥dulo se integra en el Marketplace Admin existente:

- **Navegaci√≥n**: Nueva secci√≥n "Sistema de Scraping" en el sidebar
- **Routing**: Rutas bajo `/scraper/*`
- **Styling**: Usa el sistema de dise√±o compartido (Tailwind + shadcn/ui)
- **Estado**: Manejo local con React hooks y polling para actualizaciones

## üì¶ Componentes

### ScraperDashboard

Dashboard principal que muestra m√©tricas agregadas del sistema.

**Props:**
```typescript
interface ScraperDashboardProps {
  metrics: ScraperDashboardMetrics;
  activeJobs: ScraperJob[];
  onRefresh: () => void;
  loading?: boolean;
}
```

**M√©tricas mostradas:**
- Productos totales y nuevos hoy
- Fuentes activas y jobs en progreso
- Tasa de √©xito global
- Duplicados detectados
- Rendimiento por fuente (top 5)

### JobMonitor

Componente para monitorear y controlar jobs de scraping en tiempo real.

**Funcionalidades:**
- Iniciar nuevos jobs de scraping
- Ver progreso en tiempo real
- Cancelar jobs activos
- Mostrar errores y estad√≠sticas

**Props:**
```typescript
interface JobMonitorProps {
  targets: ScraperTarget[];
  onJobComplete?: (job: ScraperJob) => void;
}
```

### SourceManager

Gesti√≥n completa de fuentes de scraping con vista tipo grid.

**Caracter√≠sticas:**
- Toggle para habilitar/deshabilitar fuentes
- M√©tricas de salud por fuente (tasa de √©xito)
- Ejecuci√≥n manual de scraping
- Filtrado por nombre
- Badges de frecuencia y prioridad

### ScheduleConfig

Configuraci√≥n de programaci√≥n autom√°tica para cada fuente.

**Opciones de configuraci√≥n:**
- Frecuencia: Diario, cada 2-3 d√≠as, semanal
- Hora y minutos espec√≠ficos
- Toggle de habilitaci√≥n
- Vista previa de pr√≥xima ejecuci√≥n

### ScrapingHistory

Tabla completa con historial de todas las ejecuciones.

**Funcionalidades:**
- Filtros por estado, fecha y fuente
- Paginaci√≥n
- Exportaci√≥n a CSV
- Detalles de errores
- Estad√≠sticas por ejecuci√≥n

## üîå API Integration

### Cliente API (scraper-api.ts)

El cliente API maneja toda la comunicaci√≥n con el servicio de scraping Python.

#### Endpoints principales:

```typescript
// Dashboard metrics
GET /monitoring/health/summary

// Active jobs
GET /jobs
GET /jobs/{job_id}
DELETE /jobs/{job_id}

// Start scraping
POST /scrape
{
  target: string;
  urls?: string[];
  force?: boolean;
  metadata?: Record<string, any>;
}

// Targets
GET /targets

// Schedules
GET /monitoring/schedules
PUT /monitoring/schedules/{target_name}

// History
GET /monitoring/history?page=1&page_size=20
```

### Tipos de Datos

```typescript
interface ScraperDashboardMetrics {
  total_products: number;
  new_today: number;
  duplicates_detected: number;
  success_rate: number;
  last_run: string;
  active_sources: number;
  jobs_in_progress: number;
  by_source: Record<string, {
    products: number;
    last_run: string;
    success_rate: number;
  }>;
}

interface ScraperJob {
  job_id: string;
  status: 'pending' | 'running' | 'completed' | 'failed' | 'cancelled';
  progress: number;
  target_name: string;
  products_found: number;
  duplicates_detected: number;
  errors: string[];
  started_at: string;
  completed_at?: string;
}

interface ScraperTarget {
  name: string;
  display_name: string;
  enabled: boolean;
  url: string;
  last_run?: string;
  products_count: number;
  success_rate: number;
  frequency: 'daily' | 'every_2_days' | 'every_3_days' | 'weekly';
  priority: 'high' | 'medium' | 'low';
}
```

## üìñ Gu√≠a de Uso

### Acceso al M√≥dulo

1. Navegar al Marketplace Admin
2. En el sidebar, hacer clic en "Sistema de Scraping"
3. Se mostrar√° el dashboard principal

### Dashboard Principal

El dashboard muestra un resumen del estado actual:
- **M√©tricas generales**: Total de productos, nuevos hoy, etc.
- **Jobs activos**: Trabajos en progreso con barra de progreso
- **Rendimiento por fuente**: Top 5 fuentes con mejor rendimiento

### Gesti√≥n de Fuentes

1. Ir a la pesta√±a "Fuentes"
2. Ver todas las fuentes disponibles en formato grid
3. Acciones disponibles:
   - **Toggle switch**: Habilitar/deshabilitar fuente
   - **Ejecutar ahora**: Iniciar scraping manual
   - **Configurar**: Ajustar programaci√≥n (√≠cono calendario)

### Programaci√≥n de Scraping

1. Ir a la pesta√±a "Programaci√≥n"
2. Para cada fuente configurar:
   - **Frecuencia**: Diario, cada 2-3 d√≠as, semanal
   - **Hora**: Seleccionar hora del d√≠a
   - **Minutos**: 00, 15, 30 o 45
3. Guardar configuraci√≥n

### Consultar Historial

1. Ir a la pesta√±a "Historial"
2. Usar filtros para buscar:
   - Por fuente espec√≠fica
   - Por estado (completado, fallido, cancelado)
   - Por rango de fechas
3. Exportar resultados a CSV si es necesario

## üõ†Ô∏è Desarrollo

### Instalaci√≥n de Dependencias

```bash
cd services/saas-mt-marketplace-admin
npm install
```

### Variables de Entorno

Configurar en `.env.local`:

```env
NEXT_PUBLIC_SCRAPER_SERVICE_URL=http://localhost:8086
```

### Ejecutar en Desarrollo

```bash
npm run dev
```

El m√≥dulo estar√° disponible en: http://localhost:3004/scraper

### Agregar Nuevas Fuentes

Para agregar una nueva fuente de scraping:

1. Configurar en el servicio Python (scraper-service)
2. La fuente aparecer√° autom√°ticamente en el frontend
3. Configurar programaci√≥n seg√∫n necesidad

### Personalizaci√≥n

#### Cambiar intervalo de actualizaci√≥n

En `useScraperDashboard.ts`:

```typescript
const [refreshInterval, setRefreshInterval] = useState<number>(5000); // 5 segundos
```

#### Modificar cantidad de fuentes mostradas

En `ScraperDashboard.tsx`:

```typescript
{Object.entries(metrics.by_source).slice(0, 5).map(...)} // Cambiar 5 por el n√∫mero deseado
```

## ‚öôÔ∏è Configuraci√≥n

### Configuraci√≥n del Servicio Backend

El servicio de scraping Python debe estar ejecut√°ndose en el puerto 8086.

```bash
cd services/saas-mt-scraper-service
python -m uvicorn src.api.server:app --port 8086
```

### MongoDB

Asegurarse de que MongoDB est√© corriendo para almacenar los productos scrapeados.

### Horarios de Scraping

Los horarios est√°n configurados en hora local de Argentina (UTC-3):
- **Alta prioridad**: 9:00, 12:00, 15:00
- **Media prioridad**: 10:00, 14:00, 18:00
- **Baja prioridad**: 11:00, 16:00, 20:00

## üîß Troubleshooting

### El dashboard no muestra datos

1. Verificar que el servicio de scraping est√© corriendo:
   ```bash
   curl http://localhost:8086/health
   ```

2. Verificar la configuraci√≥n de CORS en el servicio Python

3. Revisar la consola del navegador para errores

### Jobs quedan colgados

1. Verificar logs del servicio de scraping:
   ```bash
   tail -f services/saas-mt-scraper-service/logs/scraper_*.log
   ```

2. Cancelar job manualmente desde el JobMonitor

3. Reiniciar el servicio si es necesario

### No se actualizan las m√©tricas

1. Verificar que el polling est√© activo (cada 5 segundos)
2. Refrescar manualmente con el bot√≥n "Actualizar"
3. Verificar conexi√≥n con el backend

### Error al programar scraping

1. Verificar formato de cron expression
2. Asegurar que la fuente est√© habilitada
3. Revisar permisos del usuario

## üöÄ Mejoras Futuras

### WebSocket Integration
- Reemplazar polling por WebSocket para actualizaciones en tiempo real
- Notificaciones push cuando terminen los jobs

### An√°lisis Avanzado
- Gr√°ficos de tendencias hist√≥ricas
- Predicci√≥n de tiempos de scraping
- Alertas autom√°ticas por anomal√≠as

### Gesti√≥n de Datos
- Vista previa de productos scrapeados
- Aprobaci√≥n manual antes de importar
- Deduplicaci√≥n inteligente

### Integraci√≥n con AI
- Curaci√≥n autom√°tica de productos
- Detecci√≥n de cambios de precios
- Categorizaci√≥n inteligente

---

## üìû Soporte

Para problemas o preguntas sobre el m√≥dulo de scraping:

1. Revisar esta documentaci√≥n
2. Consultar logs del servicio
3. Contactar al equipo de desarrollo

**√öltima actualizaci√≥n**: 31 de Enero de 2025  
**Versi√≥n**: 1.0.0